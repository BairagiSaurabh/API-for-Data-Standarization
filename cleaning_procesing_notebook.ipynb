{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feature_engine\n",
    "import numpy as np\n",
    "#import streamlit as st\n",
    "import pandas as pd\n",
    "import dtale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.imputation import (\n",
    "    MeanMedianImputer,\n",
    "    CategoricalImputer,\n",
    "    ArbitraryNumberImputer\n",
    ")\n",
    "from feature_engine.encoding import WoEEncoder\n",
    "from feature_engine.selection import DropConstantFeatures\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "from feature_engine.outliers import Winsorizer, ArbitraryOutlierCapper, OutlierTrimmer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data and its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 18)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dirty_deputies_v2.csv\",na_values='Nan')\n",
    "df.shape\n",
    "\n",
    "df = df.sample(5000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features having x% null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_with_high_nan(df, threshold):\n",
    "    nan_percentages = df.isnull().mean() * 100\n",
    "    columns_to_drop = nan_percentages[nan_percentages > threshold].index\n",
    "    print(columns_to_drop)\n",
    "    df_dropna = df.drop(columns=columns_to_drop)\n",
    "    return df_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['party_ideology4'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = drop_columns_with_high_nan(df, 60)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4991, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change datatypes if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4991 entries, 254361 to 46014\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   deputy_name         4991 non-null   object \n",
      " 1   deputy_state        4980 non-null   object \n",
      " 2   political_party     4980 non-null   object \n",
      " 3   refund_description  4991 non-null   object \n",
      " 4   company_name        4991 non-null   object \n",
      " 5   company_id          4326 non-null   float64\n",
      " 6   refund_date         4900 non-null   object \n",
      " 7   refund_value        4991 non-null   float64\n",
      " 8   party_pg            4980 non-null   object \n",
      " 9   party_en            4980 non-null   object \n",
      " 10  party_tse           4980 non-null   float64\n",
      " 11  party_regdate       4980 non-null   object \n",
      " 12  party_nmembers      4980 non-null   float64\n",
      " 13  party_ideology1     4980 non-null   object \n",
      " 14  party_ideology2     4233 non-null   object \n",
      " 15  party_ideology3     2463 non-null   object \n",
      " 16  party_position      4977 non-null   object \n",
      "dtypes: float64(4), object(13)\n",
      "memory usage: 701.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_datatype(df, column_name, new_datatype):\n",
    "\n",
    "    try:\n",
    "        if new_datatype == 'int':\n",
    "            df[column_name] = pd.to_numeric(df[column_name], errors='coerce').astype(int)\n",
    "        elif new_datatype == 'float':\n",
    "            df[column_name] = pd.to_numeric(df[column_name], errors='coerce').astype(float)\n",
    "        elif new_datatype == 'str':\n",
    "            df[column_name] = df[column_name].astype(str)\n",
    "        elif new_datatype == 'bool':\n",
    "            df[column_name] = df[column_name].astype(bool)\n",
    "        elif new_datatype == 'datetime':\n",
    "            df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "        else:\n",
    "            print(\"Unsupported data type.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting column '{column_name}' to {new_datatype} : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting column 'party_tse' to int : Cannot convert non-finite values (NA or inf) to integer\n",
      "Error converting column 'party_nmembers' to int : Cannot convert non-finite values (NA or inf) to integer\n"
     ]
    }
   ],
   "source": [
    "change_datatype(df,'company_id','str')\n",
    "change_datatype(df,'party_regdate','datetime')\n",
    "change_datatype(df,'refund_date','datetime')\n",
    "change_datatype(df,'party_tse','int')\n",
    "change_datatype(df,'party_nmembers','int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4991 entries, 254361 to 46014\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   deputy_name         4991 non-null   object        \n",
      " 1   deputy_state        4980 non-null   object        \n",
      " 2   political_party     4980 non-null   object        \n",
      " 3   refund_description  4991 non-null   object        \n",
      " 4   company_name        4991 non-null   object        \n",
      " 5   company_id          4991 non-null   object        \n",
      " 6   refund_date         4497 non-null   datetime64[ns]\n",
      " 7   refund_value        4991 non-null   float64       \n",
      " 8   party_pg            4980 non-null   object        \n",
      " 9   party_en            4980 non-null   object        \n",
      " 10  party_tse           4980 non-null   float64       \n",
      " 11  party_regdate       4980 non-null   datetime64[ns]\n",
      " 12  party_nmembers      4980 non-null   float64       \n",
      " 13  party_ideology1     4980 non-null   object        \n",
      " 14  party_ideology2     4233 non-null   object        \n",
      " 15  party_ideology3     2463 non-null   object        \n",
      " 16  party_position      4977 non-null   object        \n",
      "dtypes: datetime64[ns](2), float64(3), object(12)\n",
      "memory usage: 701.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df,target_column,test_size=0.2):\n",
    "    X = df.drop(columns=[target_column]) \n",
    "    y = df[target_column] \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    xy_test = pd.concat([X_test,y_test],axis=1)\n",
    "    xy_test.dropna(inplace=True)\n",
    "    X_test = xy_test.drop(columns=[target_column]) \n",
    "    y_test = xy_test[target_column]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(df,\"party_position\",test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (X): (3992, 16)\n",
      "Testing set (X): (449, 16)\n",
      "Training set (y): (3992,)\n",
      "Testing set (y): (449,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set (X):\", X_train.shape)\n",
    "print(\"Testing set (X):\", X_test.shape)\n",
    "print(\"Training set (y):\", y_train.shape)\n",
    "print(\"Testing set (y):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA through Dtale Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eda = pd.concat([X_train,y_train],axis=1)\n",
    "d = dtale.show(train_eda)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d._url #(get url for the browser)\n",
    "\n",
    "## Shutting down D-Tale process\n",
    "#d.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Low variance features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_variance_features(X_train,X_test,variables=None,threshold=0.9):\n",
    "    earlier_cols = list(X_train.columns)\n",
    "    dcf = DropConstantFeatures(tol = threshold,missing_values='ignore',variables=variables)\n",
    "    X_train_ = dcf.fit_transform(X_train)\n",
    "    later_cols = list(X_train_.columns)\n",
    "    diff = list(set(earlier_cols) - set(later_cols))\n",
    "    X_test_ = X_test.drop(diff,axis=1)\n",
    "    return X_train_,X_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = low_variance_features(X_train,X_test,variables=None,threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3992, 16), (449, 16))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Highly correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_correlation_features(X_train,X_test,variables=None,threshold=0.9):\n",
    "    earlier_cols = list(X_train.columns)\n",
    "    dcf = DropCorrelatedFeatures(threshold = threshold,missing_values='ignore',variables=variables)\n",
    "    X_train_ = dcf.fit_transform(X_train)\n",
    "    later_cols = list(X_train_.columns)\n",
    "    diff = list(set(earlier_cols) - set(later_cols))\n",
    "    X_test_ = X_test.drop(diff,axis=1)\n",
    "    return X_train_,X_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = high_correlation_features(X_train,X_test,variables=None,threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3992, 16), (449, 16))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols = X_train.columns[X_train.isnull().any()].tolist() # this returns all columns having null columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_datatypes(df, columns):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary of object and numerical type columns\n",
    "    \"\"\"\n",
    "    result = {'object': [], 'numerical': []}\n",
    "    for column in columns:\n",
    "        dtype = df[column].dtype\n",
    "        if dtype == 'object':\n",
    "            result['object'].append(column)\n",
    "        elif dtype == 'int64' or dtype == 'float64':\n",
    "            result['numerical'].append(column)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_dict = column_datatypes(X_train,null_cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_missing_imputation(df,only_numerical,method = \"median\"):\n",
    "    mmi = MeanMedianImputer(imputation_method=method,variables=only_numerical)\n",
    "    mmi.fit(df[only_numerical])\n",
    "    df[only_numerical] = mmi.transform(df[only_numerical])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_numerical = impute_dict['numerical']\n",
    "X_train = numerical_missing_imputation(X_train,only_numerical,method = \"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_missing_imputation(df,variables,method = \"frequent\",fill_value=None):\n",
    "    if method == \"frequent\":\n",
    "        ci = CategoricalImputer(imputation_method='frequent',variables=variables)\n",
    "        ci.fit(df[variables])\n",
    "        df[variables] = ci.transform(df[variables])\n",
    "        return df\n",
    "    elif method == \"missing\":\n",
    "        ci = CategoricalImputer(imputation_method='missing',fill_value=fill_value ,variables=variables)\n",
    "        ci.fit(df[variables])\n",
    "        df[variables] = ci.transform(df[variables])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_variables = ['deputy_state', 'political_party','party_pg','party_en','party_ideology1']\n",
    "empty_string_variables = [\"party_ideology2\",\"party_ideology3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = categorical_missing_imputation(X_train,frequent_variables,method = \"frequent\")\n",
    "X_train = categorical_missing_imputation(X_train,empty_string_variables,method = \"missing\",fill_value=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_imputation(df,outliers_list):\n",
    "    wz = Winsorizer(capping_method='gaussian', tail='both', fold=3,variables=outliers_list)\n",
    "    wz.fit(df[outliers_list])\n",
    "    df[outliers_list] = wz.transform(df[outliers_list])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_list = impute_dict[\"numerical\"]\n",
    "X_train = outlier_imputation(X_train,outliers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'object': [], 'numerical': []}\n",
    "for column in list(X_train.columns):\n",
    "    dtype = X_train[column].dtype\n",
    "    if dtype == 'object':\n",
    "        result['object'].append(column)\n",
    "    elif dtype == 'int64' or dtype == 'float64':\n",
    "        result['numerical'].append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dict = {}\n",
    "for i in result[\"object\"]:\n",
    "    encode_dict[i] = len(list(pd.unique(df[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_encoding(X_train,X_test,dict_,unique_count=30):\n",
    "    OHE_cols = [x for x in dict_ if dict_[x]<=unique_count]\n",
    "    frequency_count_cols = [x for x in dict_ if dict_[x]>unique_count]\n",
    "    #print(frequency_count_cols)\n",
    "\n",
    "    if len(OHE_cols)+len(frequency_count_cols)>2:\n",
    "        print(\"OHE & freq\",True)\n",
    "        encoder_ohe = OneHotEncoder(variables=OHE_cols,ignore_format=True)\n",
    "        X_train = encoder_ohe.fit_transform(X_train)\n",
    "        X_test = encoder_ohe.transform(X_test)\n",
    "\n",
    "        encoder_freq = CountFrequencyEncoder(encoding_method='frequency',variables=frequency_count_cols,ignore_format=True)\n",
    "        X_train = encoder_freq.fit_transform(X_train)\n",
    "        X_test = encoder_freq.transform(X_test)\n",
    "        return X_train,X_test\n",
    "\n",
    "    elif len(frequency_count_cols)>0 and len(OHE_cols)==0:\n",
    "        print(\"count_f\",True)\n",
    "        encoder_freq = CountFrequencyEncoder(encoding_method='frequency',variables=frequency_count_cols,ignore_format=True)\n",
    "        X_train = encoder_freq.fit_transform(X_train)\n",
    "        X_test = encoder_freq.transform(X_test)\n",
    "        return X_train,X_test\n",
    "    \n",
    "    elif len(frequency_count_cols)>0 and len(OHE_cols)==0:\n",
    "        print(\"OHE\",True)\n",
    "        encoder_ohe = OneHotEncoder(variables=OHE_cols,ignore_format=True)\n",
    "        X_train = encoder_ohe.fit_transform(X_train)\n",
    "        X_test = encoder_ohe.transform(X_test)\n",
    "        return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHE & freq True\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test = categorical_encoding(X_train,X_test,encode_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encoding(Xy_train,Xy_test,date_col,target_column = list(pd.DataFrame(y_train).columns)[0]):\n",
    "    Xy_train[\"Month\"] = pd.DatetimeIndex(Xy_train[date_col]).month \n",
    "    Xy_train[\"Day\"] = pd.DatetimeIndex(Xy_train[date_col]).day\n",
    "\n",
    "    Xy_test[\"Month\"] = pd.DatetimeIndex(Xy_test[date_col]).month \n",
    "    Xy_test[\"Day\"] = pd.DatetimeIndex(Xy_test[date_col]).day\n",
    "\n",
    "    Xy_train.dropna(subset=[date_col],inplace=True)\n",
    "    Xy_test.dropna(subset=[date_col],inplace=True)\n",
    "\n",
    "    X_train = Xy_train.drop(columns=[target_column]) \n",
    "    y_train = Xy_train[target_column]\n",
    "\n",
    "    X_test = Xy_test.drop(columns=[target_column]) \n",
    "    y_test = Xy_test[target_column]\n",
    "\n",
    "    cyclical = CyclicalFeatures(variables=[\"Month\",\"Day\"], drop_original=True)\n",
    "    X_train = cyclical.fit_transform(X_train)\n",
    "    X_test = cyclical.transform(X_test)\n",
    "\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = pd.concat([X_train,y_train],axis=1)\n",
    "Xy_test = pd.concat([X_test,y_test],axis=1)\n",
    "X_train,X_test,y_train,y_test = cyclical_encoding(Xy_train,Xy_test,\"party_regdate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize/standardize the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train,X_test, features, method='normalization'):\n",
    "\n",
    "    if method == 'normalization':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'standardization':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported scaling method. Please use 'normalization' or 'standardization'.\")\n",
    "\n",
    "    X_train[features] = scaler.fit_transform(X_train[features])\n",
    "    X_test[features] = scaler.transform(X_test[features])\n",
    "\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = scale_features(X_train,X_test, result['numerical'], method='normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encode target class (OHE/Labelencoder):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target_variable(y_train, y_test, method='label_encoding'):\n",
    "    if method == 'label_encoding':\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "        y_test_encoded = label_encoder.transform(y_test)\n",
    "        return pd.DataFrame(y_train_encoded), pd.DataFrame(y_test_encoded)\n",
    "    \n",
    "    elif method == 'one_hot_encoding':\n",
    "        one_hot_encoder = OneHotEncoder(ignore_format=False)\n",
    "        y_train_encoded = one_hot_encoder.fit_transform(pd.DataFrame(y_train))\n",
    "        y_test_encoded = one_hot_encoder.transform(pd.DataFrame(y_test))\n",
    "        return y_train_encoded, y_test_encoded\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported encoding method. Please use 'label_encoding' or 'one_hot_encoding'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded, y_test_encoded= encode_target_variable(y_train, y_test, method='label_encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (X): (3982, 185)\n",
      "Testing set (X): (449, 185)\n",
      "Training set (y): (3982, 1)\n",
      "Testing set (y): (449, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set (X):\", X_train.shape)\n",
    "print(\"Testing set (X):\", X_test.shape)\n",
    "print(\"Training set (y):\", y_train_encoded.shape)\n",
    "print(\"Testing set (y):\", y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"X_train.csv\")\n",
    "X_test.to_csv(\"X_test.csv\")\n",
    "y_train_encoded.to_csv(\"y_train.csv\")\n",
    "y_test_encoded.to_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
